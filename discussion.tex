Immersive technologies occupy a space that intersects emerging, enabling, and entrenched technologies. While we have some concrete data and examples of misuse, we are also in a speculative position---what is the potential of these technologies and how can it be abused? As a platform, the immersive web has less specific considerations than stand-alone technologies that will be built on top of it. For example, an education app may ask ``how can we create a virtual classroom without violating students' privacy.'' They would plausibly consider minimizing collection of biometric data that could inadvertently reveal health conditions and whether it is acceptable to infer that a student's attention has wandered and manipulate them back to the task at hand. As a platform, we need to consider how we build thoughtful controls for accessing the biometric data that could enable this.

This means that we also need to understand the current legal concept of privacy and how it might apply to future technologies. By identifying gaps at this point in time, we can collaborate with legislators to craft regulations that are useful and technically feasible. For example, instituting a blanket ban on collecting biometric data would be less useful than preventing companies from selling information to third parties that is been derived from the biometric data.

A key issue for analyzing legal implications identified by Lemley and Volokh is the \emph{Bangladesh Problem}, which is the concept that violators or perpetrators of `virtual crimes' can reside in completely different jurisdictions from those of their victims~\cite{lemley2017law}. While this is true for nearly all similar internet-enabled technologies, the presence and data collection capabilities (and ensuing opportunity for privacy or other violations) inherent in MR technologies offers new urgency in analyzing current and future legal doctrine. These laws should be informed by both ethical principles as the ones listed here and the researchers and implementors of MR technologies.

\subsection{Current legal landscape}\label{sec:laws}
%Overview of information privacy history that we will reference
%- 'right to privacy' as penumbra
%- warren and brandeis
%- Whalen v Roe and 'right to information privacy'

With the explosion of technical advances that have occurred over the past few decades, it is not surprising that the law has struggled to keep pace. In his \emph{Olmstead v United States} dissent, Justice Louis Brandeis states that ``[the government] relies on the language of the [Fourth] amendment, and it claims that the protection given thereby cannot properly be held to include a telephone conversation... this Court has repeatedly sustained the exercise of power by Congress, under various clauses of that instrument, over objects of which the Fathers could not have dreamed.''

Brandeis' argument is that the Fourth Amendment still protects technologies that did not exist when it was conceived and written. The amendment's protections extend to new technologies; otherwise, the government would have the ability to violate privacy in more subtle ways than forcible search and seizure.

Privacy is not just about what the law says---it is shaped by the society in which we live and how we value privacy. Ethics and society can inform the law, and in turn, the law should reinforce ethical and social norms. While some aspects of MR devices are inadequately addressed by current laws, there are some clear parallels we can make.

Consider the `emotion-annotating'  HMD presented in \autoref{subsec:communications} and its resemblance to polygraph machines. Modern polygraphs use similar sensors to those used in MR devices, including GSR,\footnote{galvanic skin response} blood pressure, and respiration, to detect if subjects are being untruthful. However, most courts exclude polygraphs as evidence, citing weak scientific underpinnings. Despite this, many employers still attempt to use them during hiring or investigations. Thanks to the Employee Polygraph Protection Act~\cite{polygraph}, private sector employers are disallowed from requiring polygraphs or disciplining employees solely on the basis of the results of a polygraph test, but there are some circumstances where it is lawful to use them.

There are limits on what questions can be asked by polygraph examiners, namely questions regarding religion, race, politics, sexual behavior, or union activities. Based on this existing precedent, we can say that the law \emph{should} similarly prevent the use of MR devices to infer those beliefs. Unfortunately, it may be more difficult to prove that an MR device inferred this information than it is to show that a polygraph examiner asked a question illegally.

The US Constitution does not explicitly state a right to privacy, but the courts have asserted that it can be inferred by the First, Third, Fourth, and Fifth amendments. However, this only applies to intrusions by the government; instead, private sector data collection and use are regulated by a patchwork of federal and state laws. Instead of blanket privacy laws, sector-specific privacy laws are more common, such as the Video Privacy Protection Act and the Children's Online Privacy Protection Act.

\begin{quote}
The sectoral approach in the United States can sometimes draw even finer distinctions for similar kinds of information. For example, cable TV records are regulated differently from video rental or sale records. There are no industry specific federal statutes directed towards the personal information contained in the records of most merchants.\cite{solove2018information}
\end{quote} 

\subsubsection{Personally identifiable information (PII)}
The concept of PII is central to most privacy legislation, so, what is it? We do not actually have a good definition.
\begin{description}
	\item[Tautological]: ``PII is any information that identifies a person''
	\item[Non-public]: PII is non-public information (but does not mention identifiability)
	\item[Specific types]: lists data that is PII, such as name and address
\end{description}

I contend that the tautological approach is the least bad of these, because it is more adaptable to new technologies. For example, suppose an individual's movements in a virtual world create a unique, identifying fingerprint. Would that be PII? It would not be listed as PII in legislation, and it is not necessarily `non-public,' but it is an identifying biometric that we have an interest in protecting. Defining, addressing, and protecting PII in the immersive world are open questions.


\subsubsection{Reasonable expectation of privacy}
The ``reasonable expectation of privacy'' test was established in a concurring opinion for \emph{Katz v United States}\cite{katz1967} to articulate when the Fourth Amendment applies:
\begin{enumerate}
	\item A person must exhibit an ``actual (subjective) expectation of privacy''
	\item ``The expectation [must] be one that society is  prepared to recognize as `reasonable'''
\end{enumerate}

The Court ruled that it was a violation for the police to record Katz's conversations in a phone booth, noting that ``the Fourth Amendment protects people, not places.'' When we look at applying these principles to MR technology and experiences, we do not have well-defined concept of what privacy is in the space. As a society, we need to determine what `reasonable privacy' is in virtual spaces. This is complicated by the \emph{Third Party Doctrine} established by \emph{Smith v Maryland}~\cite{1979smith}, which holds that individuals have no reasonable expectation of privacy over data they have `voluntarily' given to third parties.

\begin{quote}
Marshall, J. joined by Brennan, J. dissenting \ldots {rivacy is not a discrete commodity, possessed absolutely or not at all \ldots In so ruling, the Court determines that individuals who convey information to third parties have `assumed the risk' of disclosure to the government. 
\end{quote}

The Third Party Doctrine has not adapted well to the digital age---should we be forced to forgo privacy because companies collect massive amounts of personal and behavioral data on us? Justice Sonia Sotomayor argues this in her concurring opinion for \emph{United States v Jones}~\cite{jones2012}. Solove contends that maintaining the Third Part Doctrine in a world where businesses maintain detailed digital dossiers will violate individual's First Amendment rights and leave them vulnerable to government abuses~\cite{solove2001digital}. This also raises concerns with the concept that individuals are voluntarily providing data to third parties. Is it possible to make a phone call without providing the number to your provider? Or to browse the internet without giving data to your ISP? These are common ways we participate in society; it is unethical to require that individuals avoid them to preserve their privacy.

Consider the role of this doctrine in the immersive age. Companies will have to collect information on our gaze, motion, and physical responses to enable certain MR experiences. Are we prepared to give up the ability to make decisions without constantly worrying that the government is monitoring our eye-tracking data (revealing internal thought processes)? Instead, should companies be forbidden from misusing or disclosing this data? 

% Katz v United States 389 US 347 (1967)
% Smith v Maryland 442 US 735 (1979)
% United States v. Jones, 565 U.S. ___, 132 S. Ct. 945, 957 (2012) (Sotomayor, J., concurring).

\subsubsection{Behavioral data and marketing}

Targeted marketing often avoids the privacy problem by asserting that the behavioral data they use is not PII and users are anonymized. However, this ignores the power of technology to deanonymize such information~\cite{narayanan2008robust}. In \emph{NAACP v. Alabama}~\cite{1958naacp}, the Court ruled that compelling the NAACP to disclose information about members would violate their right to freedom of association, because it depends on their privacy. If their membership was revealed, this could lead to ostracism. MR provides even richer behavioral and biometric data sources. We have not yet explored deanonymization in the space, but we expect it will be possible.

\subsubsection{Privacy Policies}

Most companies post privacy policies on their webpages. In these, it is common for companies to state that they can use and disclose personal information however they would like unless the consumer opts out, emphasizing the role of individual choice in US privacy management. However, Daniel Solove ``contend[s] that [privacy self-management] is being tasked with doing work beyond its capabilities....[and] does not provide people with meaningful control over their data''~\cite{solove2012introduction}. In particular, the use of big data analytics precludes non-experts from understanding many privacy implications~\cite{baruh2017big}. This is not a failure of individuals' comprehension, but rather a deliberate barrier---companies have a vested interest in maintaining their access to behavioral marketing and selling data to third parties.

In addition to informing users (although, in practice, consumers rarely read privacy policies), privacy policies have become a way to make companies accountable. The Federal Trade Commission (FTC) considers violating privacy policies to be an unfair and deceptive practice. This has led to the FTC being the leading privacy agency in the US, despite being limited in enforcement capabilities.

\subsubsection{California Consumer Protection Act}
The California Consumer Protection Act (CCPA) came into effect on January 1, 2020, protecting the data of all California residents. It uses a tautological definition of PII, defining ``personal information'' as ``any information that \ldots relates to \ldots a particular consumer or household.'' This includes data such as names, Internet Protocol addresses, and web browsing histories.\footnote{\url{https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=201720180AB375}}
 From an MR perspective, we can expect that this will broadly cover MR data outputs.

\subsubsection{International Privacy Law}

American laws are focused on protecting the individual's domain (particularly their home) from the state, while European laws treat privacy as fundamental part of human dignity.
In contrast to the US's sectoral approach to privacy, Europe has crafted comprehensive information privacy laws. The European Union's new data privacy law, the General Data Protection Regulation (GDPR) creates strong legal protections for individual rights, limits processing and collection of sensitive data, and increases enforcement tools, like fines. A key element of the GDPR is disallowing individuals from `opting out' of the fundamental protections to prevent companies from gathering and processing data that is beyond the purpose of the contract.

The complexities introduced by requiring companies to comply with different privacy laws across the world (some of which are directly opposed to others) will only get worse as immersive technologies make the world even smaller.



\subsection{Concrete steps for ethical decision making}\label{sec:steps}
\subsubsection{Educate and assist lawmakers}
If technologists do not coordinate with legislators, we will end up with rigid regulations that create a lot of paperwork, but end up being privacy theater. This is exacerbated in the US, where states have been leading the charge for better privacy legislation, forcing companies to comply with a hodgepodge of laws.

By working with legislators, we can help them understand the complications and unique considerations for mixed reality technology. We can also try to craft legislation that will not create an undue burden on smaller companies---companies spent millions of dollars retrofitting their systems for GDPR compliance. Some companies chose not to comply and stopped serving European users to avoid penalties.

\subsubsection{Establish a regulatory authority for flexible and responsive oversight}

It is unrealistic to expect laws to keep up with the pace of technological advancement. For bills signed into law from 2011-2019, it took 241 days between for a bill to become a law. Generally, this is a good thing. Stable laws create a stable government. Unfortunately, this means that the laws are significantly behind, and they are written and interpreted by people who do not have much technical background.

The Financial Industry Regulatory Authority (FINRA) was established in 2017 to be a self-regulatory organization (SRO) for the finance industry. Its purpose is to promote trust in the US finance industry by ensuring that firms operate fairly. The tech industry can similarly embrace an SRO to create ethical guidelines and craft (and enforce) regulation to better serve consumers~\cite{reich}.

The web is shaped by standards bodies like the World Wide Web Consortium (W3C). Standards are crafted by consensus, so the intentions of a single bad actor are minimized.

When one browser has a monopoly, developers are not incentivized to make their sites work on multiple platforms. This decreases both competition and the efficacy of web standards---if there is only one major browser, then their implementation becomes synonymous with the standard. We need diverse viewpoints and interests to shape the web, otherwise it will only serve the interests of a few.

Like the model of web governance, consensus should mitigate conflicts of interest. The tech industry is broad with competing interests---while we have discovered that we can not trust when a single company says, ``Trust us with your data,'' we might be able to trust a diverse group of experts who say, ``Trust us to ensure your privacy is respected.'' Between this and government oversight, we can have a flexible and responsive regulatory authority that minimizes internal abuse.

\subsubsection{Engage engineers and designers to incorporate privacy by design}

Privacy must be a first-class requirement. However, we also need to realize that privacy is never going to be a one-size-fits-all scenario. A possible approach would be to begin with the most restrictive privacy settings, then allow users to modify the settings when they take actions that may benefit from relaxing the settings.

Privacy is as important as performance and usability. During the development process, it should be considered a requirement, not as an optional feature.

\subsubsection{Empower users to understand the risks and benefits of immersive technology}

The foundations of US privacy law turn on the idea of `reasonable expectation of privacy,' which is determined by society. Unfortunately privacy is a complex, poorly defined topic. We should help consumers understand the risks, not so that they each individually have to manually manage their privacy, but so they can advocate for themselves as a group.

Immersive technologies have amazing potential. they are already used to help reduce pain in burn patients~\cite{hoffman2001effectiveness} and assist with complex manufacturing and maintenance~\cite{palmarini2018systematic}. They give us new ways to experience the world around us and connect to our friends and families. it is unacceptable to say that using MR technology means that you have to sacrifice your privacy. We also do not want to lose out on the benefits by shunning MR because of privacy implications.

\subsubsection{Incorporate experts from other fields who have addressed similar problems}

Obviously, technology is not the only field that needs ethics. Other fields have struggled with privacy, and we can learn from them. First, we need to accept the power that we, as technologists, have: we have built communication platforms; we have created robot vacuums; we have changed how news is disseminated; we have moved shopping and banking to the internet; we have digitized health records.

Now it is time for us to include medical ethicists, sociologists, anthropologists, economists, etc. to help figure out how we can address the ethical concerns discussed here and identify new issues.

Ethics are ideals---we will fall short of them. As long as we continue to work towards them, we can create a better future, one that is more respectful autonomy, self-expression, and privacy regardless of race or socioeconomic status. Taken in isolation, these steps likely will not accomplish this, they are complementary. By combining them, we can shape a better immersive future.