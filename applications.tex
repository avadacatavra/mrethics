\subsection{Communications medium and social spaces} \label{subsec:communications}
The immersive web gives us new ways to connect and represent ourselves. In an instant, you can be 'present' somewhere on the other side of the world. It is the closest we have come to apparition or teleportation.

There has been an explosion of social VR platforms---AltspaceVR, VRchat, Facebook Spaces, Rec Room, Mozilla Hubs, and Anyland to name a few. Each takes a different approach to moderation and governance. They all have some commonalities---avatars and interactions. Maloney identifies three main ethical considerations for avatars~\cite{maloney}:
\begin{description}
	\item[ Effects of perceptual manipulations]: Immersive experiences can violate physical laws and manipulate or deform body parts. How will amputees react to having four limbs in VR, but not in the physical world? Do we need to recalibrate users to the limitations of the physical world after certain VR experiences? How can we ethically study causes and prevention of cybersickness?
	\item [Negative effects caused by your avatar]: Avatar choice can effect our self perceptions even after exiting experiences. Users have experienced increased self-objectification after embodying sexualized avatars and self-imposed stereotypes. However, avatars can also affect positive behaviors---rendering users as their "future selves" can lead to increased saving behavior. How do we balance these manipulations with informing users?
	\item [Negative effects caused by others' avatars]: Representations of avatars can lead to negative stereotype confirmation, and users are less likely to collaborate with avatars that represent diverse ethnic groups. How can platforms and communities balance self-expression while preventing hate speech and minimizing bias?
\end{description}

While harassment may fall into the category of 'negative effects caused by others' avatars,' this would be too limiting. Due to the unique nature of social VR, harassers can combine the anonymity and capability of other internet social spaces (e.g. threatening text messages, inappropriate verbal conduct) with the avatar's presence to grope or make obscene gestures. In a study of 609 VR users, 36\% of men and 49\% of women experienced sexual harassment in VR~\cite{outlaw2018}.

Unfortunately, social VR enables co-present harassment to occur regardless of physical distance, because the VR-enabled embodiment  makes harassment more intense~\cite{blackwell}.

Defining harassment and providing reporting mechanisms is ongoing, particularly since definitions of harassment are subjective. Outlaw found that the most effective tools for dealing with harassment were blocking and muting harassers. In a separate study focused on women in VR spaces, all respondents reported feeling unsafe and uncomfortable after spending 30 minutes in social VR and went out of their way to avoid attention~\cite{outlaw2017}.

MR devices can also be used to enrich a user's information about the physical world. For example, consider a headset that enriches the user's worldview with sentiment data. The headset could detect facial expressions and more subtle nonverbal cues, like pupil dilation, to determine bystanders' mood and reactions. How is this different than going into a public space and simply looking around, inferring the emotions of those around you? A key differentiator is scale---a computer could analyze the emotions of everyone in the field of view while simultaneously integrating cues that would not be detectable to most humans.

The bystanders have not had the opportunity to consent to this type of analysis. While they are in a public space, that does not mean that should not have some expectation of privacy (namely, the right to not have their face and body recorded and analyzed)? In the US, you have 'reasonable expectation of privacy' in a public space (consider the "plain view" doctrine); however, the emergence of always on cameras and microphones suggests that we may need to reevaluate what privacy is 'reasonable' given the rapid technological advances and prevalence of such devices.

The same scenario can also illustrate a different set of ethical concerns. Suppose we live in a world where HMDs are common everyday wear. Instead of relying on my device to interpret others' emotions, their devices would communicate this information to mine. In this instance, perhaps they have consented to allowing their device to collect that information on them, but does that mean the device should be allowed to transmit it?

Instead of using nonverbal cues to infer emotions, the device could use facial recognition to remind user's of a person's name and everything they should remember about the person (names of spouse and children, how you met, last topic of conversation, etc.) to avoid embarrassment at cocktail parties~\cite{wassom2014augmented}? Is this less invasive? Does the private setting of a party change the ethical considerations?

% one problem in the us is that cyber crimes are federal crimes, while harassment laws are state crimes87u76y


\subsection{Privacy}
Augmented and virtual reality create, and can only really exist, in a world where cameras and computer vision applied to their outputs are more or less ubiquitous. They require data sources that can reveal individuals' intrinsic characteristics and behaviors (e.g. pose, movement, head tracking) for minimal functionality. Defining and defending privacy in this world is an existential question for the technology.

Section \ref{sec:data} discusses some of the implications of the data MR devices collect and process in order to function. Privacy is difficult to define, because we each have different risks and threats to evaluate. A one-size-fits-all approach will not suffice. While this is a pervasive issue in tech today, MR introduces new opportunities and considerations for violating users' privacy.

It has become common to require that people pay-for-privacy. Consider shopping: if you share your address (physical or email), then you can enjoy increased discounts. Nearly 1 in 4 respondents to a holiday shopping survey said they would not hesitate to provide their personal information for better deals~\cite{moses}. The trouble is that There has so much data now, that we are not just giving an address for better deals. we are enabling companies to build and share huge user profiles. Facebook distributed an application, called Facebook Research, that paid users \$20 in giftcards per month to allow Facebook nearly unfettered access to their devices~\cite{axon}.

A notable example is the Washington Post. After the European Union passed the General Data Protection Regulation (GDPR), the Washington Post responded with two different subscriptions---\$90 for the GDPR-compliant EU version, \$60 for the US version~\cite{karl}.

\subsubsection{Scenario: Facebook Research, Oculus edition}

What could Facebook Research look like for HMDs? Facebook's Oculus devices are becoming more affordable for the masses---consider an initiative that provides free devices to low-income students. These devices offer access to learning opportunities and advanced courses that are not available in the student's local school. In exchange, they (and their parents) consent to providing Facebook with all data collected by the device. The resulting data would create a comprehensive profile of the child---their physical development, voice, educational progress, emotional maturity, etc.

In pursuing better education at a price they can afford, the child has instead paid with their current and future privacy. We can anticipate a number of possible scenarios that could follow:

Suppose that one day, the student's gait changes. Facebook's algorithms identify that the child has likely sprained their ankle and serves ads for crutches, orthopedic doctors, homeopathic cures, etc.

Now, suppose that in high school, the student struggles for a period---skipping class, getting into fights, poor grades---due to difficulties at home. Although the student's behavior and grades improve, they're labeled as a troublemaker or a low performer. Later on, they struggle to get a job, because companies pay Facebook for additional information on applicants.

\subsubsection{Scenario: Schools, VR and neuropsychological diagnosis}

Schools provide a vital, though controversial, role in children's health. In addition to teachers and nurses administering necessary medication, teachers often act as "disease-spotters" for disorders like ADHD~\cite{phillips2006medicine}. If a teacher suspects that a student may have a disorder, they would alert the parents, who would then need to have a doctor diagnose ADHD. Diagnosis is difficult, but research has shown that using VR can improve neuropsychological assessments~\cite{areces2018analysis}. As VR becomes widely deployed in classrooms, it may be tempting for schools to monitor children's interactions in virtual environments to assess whether they have these conditions.

What happens if we have schools collecting this type of data on students? Is it ethical for them to interfere in students' health like this? Will they create student profiles that indicate 'ADHD tendencies' without a formal diagnosis? How will educators treat them differently? Will it follow them throughout their entire education?

Principles 1 (ask permission, not forgiveness) and 7 (be transparent and accountable) would be best suited to prevent misuse in both of these scenarios. It is possible that some students and their families would choose to give informed consent, particularly if they were able to get regular reports on what type of data was being collected and how it was used.


\subsection{Accessibility and inclusion}

The immersive web gives us new ways to connect and represent ourselves. We need to design systems to prevent abuse and harassment as first class requirements while empowering users to choose how they are represented and recognized on the immersive web.

Current HMDs largely rely on motion controls and require users to take certain positions (i.e. standing). While we can create accessible applications on an immersive web, they're not usable if the HMD can not accommodate them. As a society, we have recognized that excluding people based on disability is as unethical as excluding them based on skin color, gender identity, or sexual orientation.

MR devices need to integrate appropriate accommodations, like controllers that provide haptic feedback~\cite{zhao2018demonstration} or settings that allow users to indicate that they're in a wheelchair so that the device does not keep insisting that they stand.

MR also has unique potential as an assistive technology. VR presence and embodiment can allow wheelchair bound users to ski~\cite{harrell} or allow elderly relatives to participate in family events (even if they can not travel). For visually impaired users, MR could read signs or papers out loud. It can also help with navigation by reading out directions in real time and detecting oncoming traffic. For hearing impaired people, MR devices can provide personal subtitles in theaters~\cite{forrest} and translate public announcements (like train announcements) into text in real time. They could also interpret group conversations into subtitles while providing speaker attribution.

Unfortunately, many accessibility features also pose privacy concerns.

\subsubsection{Scenario: Real-time 'subtitles'}

For people who are deaf or hearing-impaired, group conversations can be particularly difficult to follow. Speech processing has made it possible to have real-time captions~\cite{welch}, which could then be displayed on the user's HMD. In a group conversation, this would likely be jumbled; however, It is plausible that we can use the spatial characteristics of audio to determine who says what. Then, the captions could be displayed to indicate who said what, allowing the user to more easily follow the conversation.

What ethical concerns might exist in this situation? Is this kind of video and audio processing considered recording? Do the bystanders need to consent to this? If the data is sent to the cloud for processing, do the participants have a reasonable expectation of privacy? What happens if the device inadvertently transcribes a conversation the user was not intended to hear?

Bystander privacy is a particularly difficult problem~\cite{perez}. The current solution for alerting bystanders that they're being recorded is an indicator light, providing notice, but no choice. Perhaps a way to embrace principle 6 (privacy as a first-class requirement) from \autoref{sec:mrethics} in this situation is to perform real-time processing like this on the device.

\subsubsection{Scenario: Gaze-based navigation}

People with mobility impairments are excluded from using technology that's largely touch centric, like phones and tablets. Workarounds exist, like voice control and dedicated 'accessible apps,' but tasks that are often considered simple, like navigating a web page or answering a call, become more complex. What if we could use gaze instead? This would allow people with very limited mobility to fully navigate an immersive world. User input in MR is already largely speech based, because text input is difficult and time-consuming in HMDs.

As mentioned in section \ref{sec:biometrics}, gaze tracking is a powerful biometric. Not only can our eyes indicate sexual attraction and other nonverbal reactions, but they can reveal details about our decision making process~\cite{costandi}.

Is it ethical to require disabled users to sacrifice that much privacy in order to use modern technology?


\subsubsection{Representation}

MR should be a tool for inclusion and representation; for example, immunocompromised students could be 'present' in the classroom without risking their health.  In addition to accommodating disabilities, HMDs also need to fit over different hair styles and track different skin colors. As MR becomes more prevalent in education, the problems of excluding individuals because the HMDs don't fit properly become even more obvious.

As social MR becomes more prevalent, we need to build platforms that are flexible enough to allow individuals to shape their experiences and how they're perceived. For example, offering two gender options for avatars would prevent a nonbinary person from being adequately represented. Do avatars even need to be human? Should humanoid avatars accurately represent the physical attributes of a person? We need to balance giving individuals the option to accurately represent themselves (by providing adequate skin/hair/body options) while accepting that they may choose a different appearance. For example, a woman may choose a male avatar when entering a virtual space to avoid harassment.

Unfortunately, just like in the physical world, individuals will choose to express themselves in ways others may consider inappropriate or obscene. Suppose a user creates an avatar that displays a Nazi symbol. Hopefully, the platform would ban any displays of hate symbols, and provide easy mechanisms for reporting and banning anyone who violates that. While this might not be illegal in the US, other countries, like Germany, have laws prohibiting Nazi symbols and other forms of hate speech---does the platform have a duty to cooperate? While we develop technologies that allow us to be 'present' anywhere in the world, we need to consider how international laws and norms impact our platforms.



