This section presents an introduction to various MR application spaces and relevant case studies. As MR is still an emerging technology, some of the case studies combine related work and anticipatory foresight to present plausible scenarios, while others are based in historical fact.

\subsection{Communications medium and social spaces} \label{subsec:communications}
The immersive web gives us new ways to connect and represent ourselves. In an instant, you can be `present' somewhere on the other side of the world. It is the closest we have come to apparition or teleportation.

There has been an explosion of social VR platforms---AltspaceVR, VRchat, Facebook Spaces, Rec Room, Mozilla Hubs, and Anyland to name a few. Each takes a different approach to moderation and governance. They all have some commonalities---avatars and interactions. Maloney identifies three main ethical considerations for avatars \cite{maloney}:
\begin{description}
	\item[ Effects of perceptual manipulations]: Immersive experiences can violate physical laws and manipulate or deform body parts. How will amputees react to having four limbs in VR, but not in the physical world? Do we need to recalibrate users to the limitations of the physical world after certain VR experiences? How can we ethically study causes and prevention of cybersickness?
	\item [Negative effects caused by your avatar]: Avatar choice can effect our self perceptions even after exiting experiences. Users have experienced increased self-objectification after embodying sexualized avatars and self-imposed stereotypes. However, avatars can also affect positive behaviors---rendering users as their ``future selves" can lead to increased saving behavior. How do we balance these manipulations with informing users?
	\item [Negative effects caused by others' avatars]: Representations of avatars can lead to negative stereotype confirmation, and users are less likely to collaborate with avatars that represent diverse ethnic groups. How can platforms and communities balance self-expression while preventing hate speech and minimizing bias?
\end{description}

While harassment may fall into the category of ``negative effects caused by others' avatars,'' this would be too limiting. Due to the unique nature of social VR, harassers can combine the anonymity and capability of other internet social spaces (e.g. threatening text messages, inappropriate verbal conduct) with the avatar's presence to grope or make obscene gestures. In a study of 609 VR users, 36\% of men and 49\% of women experienced sexual harassment in VR~\cite{outlaw2018}.

Unfortunately, social VR enables co-present harassment to occur regardless of physical distance, because the VR-enabled embodiment  makes harassment more intense~\cite{blackwell}.

Defining harassment and providing reporting mechanisms is ongoing, particularly since definitions of harassment are subjective. Outlaw (2017) found that the most effective tools for dealing with harassment were blocking and muting harassers. In a separate study focused on women in VR spaces, all respondents reported feeling unsafe and uncomfortable after spending 30 minutes in social VR and went out of their way to avoid attention~\cite{outlaw2017}.

MR devices can also be used to enrich a user's information about the physical world. For example, consider a headset that enriches the user's worldview with sentiment data. The headset could detect facial expressions and more subtle nonverbal cues, like pupil dilation, to determine bystanders' mood and reactions. How is this different than going into a public space and simply looking around, inferring the emotions of those around you? A key differentiator is scale---a computer could analyze the emotions of everyone in the field of view while simultaneously integrating cues that would not be detectable to most humans.

The bystanders have not had the opportunity to consent to this type of analysis. While they are in a public space, that does not mean that should not have some expectation of privacy (namely, the right to not have their face and body recorded and analyzed)? In the US, you have ``reasonable expectation of privacy'' in a public space (consider the ``plain view'' doctrine); however, the emergence of always on cameras and microphones suggests that we may need to reevaluate what privacy is `reasonable' given the rapid technological advances and prevalence of such devices~\cite{bellotti1993design}.

The same scenario can also illustrate a different set of ethical concerns. Suppose we live in a world where HMDs are common everyday wear. Instead of relying on my device to interpret others' emotions, their devices would communicate this information to mine. In this instance, perhaps they have consented to allowing their device to collect that information on them, but does that mean the device should be allowed to transmit it?

Instead of using nonverbal cues to infer emotions, the device could use facial recognition to remind user's of a person's name and everything they should remember about the person (names of spouse and children, how you met, last topic of conversation, etc.) to avoid embarrassment at cocktail parties\cite{wassom2014augmented}? Is this less invasive? Does the private setting of a party change the ethical considerations?

% one problem in the us is that cyber crimes are federal crimes, while harassment laws are state crimes87u76y

\subsubsection{Case study: Harassment in VR}
In 2016, Jordan Belamire came forward with her account of being groped in a VR game called \emph{QuiVr}.

\begin{quote}
His floating hand approached my body, and he started to virtually rub my chest...This goaded him on, and even when I turned away from him, he chased me around, making grabbing and pinching motions near my chest. Emboldened, he even shoved his hand toward my virtual crotch and began rubbing... Of course, you’re not physically being touched, just like you’re not actually one hundred feet off the ground, but it’s still scary as hell....What had just happened? I hadn’t lasted three minutes in multi-player without getting virtually groped. 

What’s worse is that it felt real, violating.\footnote{\url{https://medium.com/athena-talks/my-first-virtual-reality-sexual-assault-2330410b62ee}}
\end{quote}

The developers of the game quickly responded, created a `personal bubble' feature, and lamented that they had not anticipated this problem in the first place.\footnote{\url{https://uploadvr.com/dealing-with-harassment-in-vr/}} With the personal bubble, when another player gets too close to an individual, they fade from view.

Belamire's op-ed highlights a few challenges when dealing with harassment in VR. First and foremost, while it may be virtual, the visceral nature of VR makes it feel real. Secondly, because it is virtual, the perpetrator might be anywhere, e.g. they could reside in any legal jurisdiction, making it difficult to punish outside of the platform. Finally, the only recourse is to self-select out of the experience, removing yourself from the environment, and potentially avoiding MR experiences altogether, depending on the severity of one's individual reaction.

Unfortunately, people behave badly, particularly when protected by the pseudo-anonymity of the internet. Individuals belonging to groups that are more likely to be victims of harassment may choose to represent themselves as part of a majority group as a risk mitigation technique, but it is still a responsibility of platforms and developers to prioritize protective and reporting measures.


% https://medium.com/athena-talks/my-first-virtual-reality-sexual-assault-2330410b62ee

\subsection{Privacy}
Augmented and virtual reality create, and can only really exist, in a world where cameras and computer vision applied to their outputs are more or less ubiquitous. They require data sources that can reveal individuals' intrinsic characteristics and behaviors (e.g. pose, movement, head tracking) for minimal functionality. Defining and defending privacy in this world is an existential question for the technology.

Section \ref{sec:data} discusses some of the implications of the data MR devices collect and process in order to function. Privacy is difficult to define, because we each have different risks and threats to evaluate. A one-size-fits-all approach will not suffice. While this is a pervasive issue in tech today, MR introduces new opportunities and considerations for violating users' privacy.

It has become common to require that people pay-for-privacy. Consider shopping: if you share your address (physical or email), then you can enjoy increased discounts. Nearly 1 in 4 respondents to a holiday shopping survey said they would not hesitate to provide their personal information for better deals~\cite{moses}. The trouble is that there is so much data now, that we are not just giving an address for better deals. We are enabling companies to build and share huge user profiles. Facebook distributed an application, called Facebook Research, that paid users \$20 in giftcards per month to allow Facebook nearly unfettered access to their devices~\cite{axon}.

A notable example is the Washington Post. After the European Union passed the General Data Protection Regulation (GDPR), the Washington Post responded with two different subscriptions---\$90 for the GDPR-compliant EU version, \$60 for the US version~\cite{karl}.

\subsubsection{Case study: Camera access and health data}\label{sec:data:world:camera}

We know that advertisers are interested in users' health data~\cite{jeong2019insurers}. Consider the classic AR example: an interior design application that places virtual furniture in your home. Users often leave medications out; it is plausible that when using this application, the application will detect the medication, identify it (either by the unique pill shape or by detecting and reading the label), then transmit this information to third parties, which will then use this information to target me for ads related to my condition.

There are a number of reasons AR apps tend to default to full camera access. Sometimes, the libraries applications depend on require more permissions than the application actually uses; however, by using the library, they must request enhanced sensor access. Most importantly, it is easier. Applications can determine what information they need and discard the remainder, or they can take the surplus data and turn it into a new revenue stream, a lucrative practice pioneered by Google~\cite{zuboff2019age}.

Google initially ignored the collateral data produced by search queries until engineers realized that this `data exhaust' could be used to model users' behavior. At first, this operated as a `behavioral value reinvestment cycle,' where Google harvested user data to improve the search product. Later on, engineers realized that this behavioral data surplus could also be used to create detailed user profiles and target ads more successfully~\cite{patent2003targetedad}.

There are numerous ethical concerns with this scenario. Is it acceptable for advertisers to target users based on medical data? Is it acceptable for applications to gather data like this, unrelated to the purpose of the application?  A legitimate use of the same data that is problematic in this instance would be an application that identifies pills and their uses, possibly as an aid for healthcare professionals.

As a platform for creating immersive applications, we are faced with a complex problem: how can we enable legitimate uses of sensitive data while discouraging misuse? One potential way to approach this specific problem is to recognize that medications and prescription labels are often highly standardized. We could use object recognition techniques to detect the labels, then not provide that information to the application unless the user explicitly grants further permissions.

%it is clear that following principles 5 (least privilege) and 6 (privacy as a first class requirement) from \autoref{sec:mrethics} is important in this scenario. Perhaps instead of simply asking for permission to use the camera, the application instead asks for permission to share medical data it detects, embracing both principles 1 (ask permission, not forgiveness) and 3 (empower individuals).

The developers of this application could prioritize privacy by using the least amount of data necessary for basic functionality of their application. Perhaps instead of simply asking for permission to use the camera, the application instead asks for permission to share medical data it detects, focusing on the inference of interest, not the data source.


%\subsubsection{Case study: Facebook Research, Oculus edition}

%What could Facebook Research look like for HMDs? Facebook's Oculus devices are becoming more affordable for the masses---consider an initiative that provides free devices to low-income students. These devices offer access to learning opportunities and advanced courses that are not available in the student's local school. In exchange, they (and their parents) consent to providing Facebook with all data collected by the device. The resulting data would create a comprehensive profile of the child---their physical development, voice, educational progress, emotional maturity, etc.

%In pursuing better education at a price they can afford, the child has instead paid with their current and future privacy. We can anticipate a number of possible scenarios that could follow:

%Suppose that one day, the student's gait changes. Facebook's algorithms identify that the child has likely sprained their ankle and serves ads for crutches, orthopedic doctors, homeopathic cures, etc.

%Now, suppose that in high school, the student struggles for a period---skipping class, getting into fights, poor grades---due to difficulties at home. Although the student's behavior and grades improve, they are labeled as a troublemaker or a low performer. Later on, they struggle to get a job, because companies pay Facebook for additional information on applicants.

\subsubsection{Case study: Schools, VR and neuropsychological diagnosis}

Schools provide a vital, though controversial, role in children's health. In addition to teachers and nurses administering necessary medication, teachers often act as ``disease-spotters'' for disorders like ADHD~\cite{phillips2006medicine}. If a teacher suspects that a student may have a disorder, they would alert the parents, who would then need to have a doctor diagnose ADHD. Diagnosis is difficult, but research has shown that using VR can improve neuropsychological assessments~\cite{areces2018analysis}. As VR becomes widely deployed in classrooms, it may be tempting for schools to monitor children's interactions in virtual environments to assess whether they have these conditions.

What happens if we have schools collecting this type of data on students? Is it ethical for them to interfere in students' health like this? Will they create student profiles that indicate `ADHD tendencies' without a formal diagnosis? How will educators treat them differently? Will it follow them throughout their entire education?

It is possible that some students and their families would choose to give informed consent, particularly if they were able to get regular reports on what type of data was being collected and how it was used. Being transparent and accountable, and asking permission would help prevent misuse in this scenario and the previous one.


\subsection{Accessibility and inclusion}

The immersive web gives us new ways to connect and represent ourselves. We need to design systems to prevent abuse and harassment as first class requirements while empowering users to choose how they are represented and recognized on the immersive web.

Current HMDs largely rely on motion controls and require users to take certain positions (i.e. standing). While we can create accessible applications on an immersive web, they are not usable if the HMD can not accommodate them. As a society, we have recognized that excluding people based on disability is as unethical as excluding them based on skin color, gender identity, or sexual orientation.

MR devices need to integrate appropriate accommodations, like controllers that provide haptic feedback~\cite{zhao2018demonstration} or settings that allow users to indicate that they are in a wheelchair so that the device does not keep insisting that they stand.

MR also has unique potential as an assistive technology. VR presence and embodiment can allow wheelchair bound users to ski~\cite{harrell} or allow elderly relatives to participate in family events (even if they can not travel). For visually impaired users, MR could read signs or papers out loud. It can also help with navigation by reading out directions in real time and detecting oncoming traffic. For hearing impaired people, MR devices can provide personal subtitles in theaters~\cite{forrest} and translate public announcements (like train announcements) into text in real time. They could also interpret group conversations into subtitles while providing speaker attribution.

Unfortunately, many accessibility features also pose privacy concerns.

\subsubsection{Case Study: Nonverbal data and job interviews}
Some companies, like Unilever, are currently deploying emotion detection technology to predict how job applicants will react to certain situations~\cite{gilliland}. Others, like Lloyd's Bank, are putting applicants in VR simulations for similar purposes~\cite{guardian2018how}. While immersive technologies can improve geographic restrictions on interviewing or working by allowing virtual colocation, there are negative implications. Consider an applicant who is interviewing virtually for a position at a company led by a CEO whose personal religious beliefs maintain that homosexuality is immoral. During the interview, the headset detects nonverbal reactions from the candidate that suggest they may be gay, and the company's algorithms (possibly opaque to the interviewer) reject the applicant.

In 2002, Renaud et al. showed that it was possible to measure sexual preferences in VR by measuring interaction with virtual naked models \cite{renaud}. In 2012, Rieger et al. demonstrated that pupil dilation patterns are a strong indicator of sexual orientation as well \cite{rieger}. While both studies are not directly applicable, because it is unlikely that any job interview would provide erotic images as part of the interview, they demonstrate that it is possible to measure subconscious sexual orientation cues in MR. it is probable that the same patterns exist when individuals are presented with non-sexualized avatars. Additionally, MR interviewing tools could be trained to detect mannerisms that are associated with homosexuality, which would almost certainly have a very high rate of false positives. Similar analysis could be done with political and religious leanings.

Would not this be illegal? In some countries, maybe. However, if the algorithm is simply trained to reject certain behaviors, not to explicitly exclude certain sexual orientations (or political/religious opinions), it might not be. After all, the interviewer did not ask about the applicant's sexuality. The technology just detected that their personality is not `suitable,' whether or not the applicant is gay or not.

This scenario highlights the broad intersections of tech ethics in the MR space. First, we have AI ethics---is it ethical to train algorithms that reject job applicants? Should such algorithms output details on the behaviors detected and decisions made (and will a human be able to understand the details)? Should we be creating algorithms that can identify sexual orientations? MR's unique contribution to this situation is the sheer amount of nonverbal data it collects in short periods of time.

This data will be misused and the consequences could be life-altering. If we do not take action now on the privacy issues presented by nonverbal data, we will either abandon promising technology altogether, or live in a dystopia.

%How could the principles presented in \autoref{sec:mrethics} ameliorate this scenario? Principles 2 (minimize tracking and fingerprinting via biometrics), 5 (least privilege), and 7 (transparent and accountable) are most relevant here. 

Most importantly, there's no need for this application to detect and analyze the candidate's nonverbal reactions---it is modern phrenology.\footnote{The study of the shape and size of the cranium to indicate individuals' psychological attributes} Minimizing the BDD tracking would ameliorate this scenario, as well ha having a transparent and accountable algorithm for interviewing that could indicate what characteristics the individual is being rejected for.


\subsubsection{Case study: Real-time `subtitles'}

For people who are deaf or hearing-impaired, group conversations can be particularly difficult to follow. Speech processing has made it possible to have real-time captions~\cite{welch}, which could then be displayed on the user's HMD. In a group conversation, this would likely be jumbled; however, It is plausible that we can use the spatial characteristics of audio to determine who says what. Then, the captions could be displayed to indicate who said what, allowing the user to more easily follow the conversation.

What ethical concerns might exist in this situation? Is this kind of video and audio processing considered recording? Do the bystanders need to consent to this? If the data is sent to the cloud for processing, do the participants have a reasonable expectation of privacy? What happens if the device inadvertently transcribes a conversation the user was not intended to hear?

Bystander privacy is a particularly difficult problem \cite{perez}. The current solution for alerting bystanders that they are being recorded is an indicator light, providing notice, but no choice. One way to embrace privacy as a first-class requirement would be to perform real-time processing on the device and then not store the data for any additional processing. %Perhaps a way to embrace principle 6 (privacy as a first-class requirement) from \autoref{sec:mrethics} in this situation is to perform real-time processing like this on the device.

\subsubsection{Case study: Gaze-based navigation}

People with mobility impairments are excluded from using technology that is largely touch centric, like phones and tablets. Workarounds exist, like voice control and dedicated `accessible apps,' but tasks that are often considered simple, like navigating a web page or answering a call, become more complex. What if we could use gaze instead? This would allow people with very limited mobility to fully navigate an immersive world. User input in MR is already largely speech based, because text input is difficult and time-consuming in HMDs.

As mentioned in section \ref{sec:biometrics}, gaze tracking is a powerful BDD vector. Not only can our eyes indicate sexual attraction and other nonverbal reactions, but they can reveal details about our decision making process~\cite{costandi}.

Is it ethical to require disabled users to sacrifice that much privacy in order to use modern technology?


\subsubsection{Case study: Representation}

MR should be a tool for inclusion and representation; for example, immunocompromised students could be `present' in the classroom without risking their health.  In addition to accommodating disabilities, HMDs also need to fit over different hair styles and track different skin colors. As MR becomes more prevalent in education, the problems of excluding individuals because the HMDs don't fit properly become even more obvious.

As social MR becomes more prevalent, we need to build platforms that are flexible enough to allow individuals to shape their experiences and how they are perceived. For example, offering two gender options for avatars would prevent a nonbinary person from being adequately represented. Do avatars even need to be human? Should humanoid avatars accurately represent the physical attributes of a person? We need to balance giving individuals the option to accurately represent themselves (by providing adequate skin/hair/body options) while accepting that they may choose a different appearance. For example, a woman may choose a male avatar when entering a virtual space to avoid harassment.

Unfortunately, just like in the physical world, individuals may choose to express themselves in ways others may consider inappropriate or obscene. Suppose a user creates an avatar that displays a Nazi symbol. Hopefully, the platform would ban any displays of hate symbols, and provide easy mechanisms for reporting and banning anyone who violates that. While this might not be illegal in the US, other countries, like Germany, have laws prohibiting Nazi symbols and other forms of hate speech---does the platform have a duty to cooperate? While we develop technologies that allow us to be 'present' anywhere in the world, we need to consider how international laws and norms impact our platforms.


