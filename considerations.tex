Why are immersive ethics any different from other technologies? Immersive technologies, whether augmented or virtual, affect our physical bodies in ways that non-immersive technologies do not. Head-mounted displays (HMDs) overlay and mix virtual elements on our senses, changing our perceptions of ourselves and our surroundings. Sometimes, this can even result in 'cybersickness.'

A key problem in MR is that the data required to provide experiences is also inherently a privacy threat. For example, while it is reasonable for users to choose to cover their cameras on their computers or phones to protect their privacy, this simply is not an option on MR devices, which rely heavily on data derived from cameras.

What about non-immersive, hand-held, AR? While it doesn't have the same physical effects on users, it shares many of the same data privacy concerns. MR experiences continuously collect and process environmental data in near-real time. In this case, the data available is much more extensive than even the intrusive data collection we've become accustomed to currently.

Spatial computing and immersive experiences expose, by necessity, information that poses a threat to privacy. To enable these technologies, we rely on many extended duration sensors. These sensors fall into three categories: biometric, orientation, and environmental.

\subsection{Biometric data}\label{sec:biometrics}
A wide range of biometrics can be collected by head-mounted displays (HMDs), some of which are non-obvious to users. In addition to eye-tracking data, we can collect information on users' gait, height, and physical/emotional reactions.

Biometric information presents particularly difficult problems. Firstly, once exposed, there's no way to retrieve or change it. Even worse, it provides methods for fingerprinting users by their physical attributes, not just their online behaviors. Biometrics also provide insight into involuntary nonverbal reactions\cite{bailenson2018protecting}. Pupil dilation and skin temperature can indicate a user's sexual attraction or orientation. Gaze tracking can expose details of medical conditions like autism or anxiety disorders. Innocuous data like facial movements during a task can classify people as high or low performers~\cite{jabon2011automatically}.

\subsubsection{Scenario: Nonverbal data and job interviews}
Some companies, like Unilever, are currently deploying emotion detection technology to predict how job applicants will react to certain situations~\cite{gilliland}. Others, like Lloyd's Bank, are putting applicants in VR simulations for similar purposes~\cite{guardian2018how}. While immersive technologies can improve geographic restrictions on interviewing or working by allowing virtual colocation, there are negative implications. Consider an applicant who is interviewing virtually for a position at a company led by a CEO whose personal religious beliefs maintain that homosexuality is immoral. During the interview, the headset detects nonverbal reactions from the candidate that suggest they may be gay, and the company's algorithms (possibly opaque to the interviewer) reject the applicant.

In 2002, Renaud et al. showed that it was possible to measure sexual preferences in VR by measuring interaction with virtual naked models \cite{renaud}. In 2012, Rieger et al. demonstrated that pupil dilation patterns are a strong indicator of sexual orientation as well \cite{rieger}. While both studies are not directly applicable, because it is unlikely that any job interview would provide erotic images as part of the interview, they demonstrate that it is possible to measure subconscious sexual orientation cues in MR. it is probable that the same patterns exist when individuals are presented with non-sexualized avatars. Additionally, MR interviewing tools could be trained to detect mannerisms that are associated with homosexuality, which would almost certainly have a very high rate of false positives. Similar analysis could be done with political and religious leanings.

would not this be illegal? In some countries, maybe. However, if the algorithm is simply trained to reject certain behaviors, not to explicitly exclude certain sexual orientations (or political/religious opinions), it might not be. After all, the interviewer didn't ask about the applicant's sexuality. The technology just detected that their personality is not 'suitable,' whether or not the applicant is gay or not.

This scenario highlights the broad intersections of tech ethics in the MR space. First, we have AI ethics---is it ethical to train algorithms that reject job applicants? Should such algorithms output details on the behaviors detected and decisions made (and will a human be able to understand the details)? Should we be creating algorithms that can identify sexual orientations? MR's unique contribution to this situation is the sheer amount of nonverbal data it collects in short periods of time.

This data will be misused and the consequences could be life-altering. If we do not take action now on the privacy issues presented by nonverbal data, we'll either abandon promising technology altogether, or live in a dystopia.

How could the principles presented in \autoref{sec:mrethics} ameliorate this scenario? Principles 2 (minimize tracking and fingerprinting via biometrics), 5 (least privilege), and 7 (transparent and accountable) are most relevant here. Most importantly, there's no need for this application to detect and analyze the candidate's nonverbal reactions---it is modern phrenology\footnote{The study of the shape and size of the cranium to indicate individuals' psychological attributes}. A transparent and accountable algorithm for interviewing could indicate what characteristics the individual is being rejected for.


\subsection{World data}

Particularly for AR applications, we need to incorporate world data to the virtual model. While data about the physical world is collected using cameras, devices do not have to provide all information to the application. Instead of providing full camera access, platforms can provide limited hit testing or a world mesh.

We need to identify ways to incentivize the principle of least privilege---it is easy to provide full camera access to applications and let them figure out what they want to use. However, that should not be the default option. World meshes and hit testing provide sufficient spatial data for many applications without also transmitting details like text.

\subsubsection{Scenario: Camera access and health data}\label{sec:data:world:camera}

We know that advertisers are interested in users' health data~\cite{jeong2019insurers}. Consider the classic AR example: an interior design application that places virtual furniture in your home. I often leave my medications on my nightstand, so that I remember to take them before bed. it is plausible that when do I am redesigning my bedroom, the application will detect the medication, identify it (either by the unique pill shape or by detecting and reading the label), then transmit this information to third parties, which will then use this information to target me for ads related to my condition.

There are a number of reasons AR apps tend to default to full camera access. Sometimes, the libraries applications depend on require more permissions than the application actually uses; however, by using the library, they must request enhanced sensor access. Most importantly, it is easier. Applications can determine what information they need and discard the remainder...or they can take the surplus data and turn it into a new revenue stream, a lucrative practice pioneered by Google~\cite{zuboff2019age}.

Google initially ignored the collateral data produced by search queries until engineers realized that this 'data exhaust' could be used to model users' behavior. At first, this operated as a 'behavioral value reinvestment cycle,' where Google harvested user data to improve the search product. Later on, engineers realized that this behavioral data surplus could also be used to create detailed user profiles and target ads more successfully~\cite{patent2003targetedad}.

There are numerous ethical concerns with this scenario. Is it acceptable for advertisers to target users based on medical data? Is it acceptable for applications to gather data like this, unrelated to the purpose of the application?  A legitimate use of the same data that is problematic in this instance would be an application that identifies pills and their uses, possibly as an aid for healthcare professionals.

As a platform for creating immersive applications, we are faced with a complex problem: how can we enable legitimate uses of sensitive data while discouraging misuse? One potential way to approach this specific problem is to recognize that medications and prescription labels are often highly standardized. We could use object recognition techniques to detect the labels, then not provide that information to the application unless the user explicitly grants further permissions.

it is clear that following principles 5 (least privilege) and 6 (privacy as a first class requirement) from \autoref{sec:mrethics} is important in this scenario. Perhaps instead of simply asking for permission to use the camera, the application instead asks for permission to share medical data it detects, embracing both principles 1 (ask permission, not forgiveness) and 3 (empower individuals).

\subsection{Orientation data}

MR devices use accelerometers, magnetometers, and gyroscopes to orient themselves in the physical world. Because of permission fatigue, devices do not ask permission for all sensor accesses. Instead, they sort sensors into two categories: dangerous and not. 'Dangerous' sensors include microphones, cameras, and GPS, while orientation sensors are considered 'not dangerous.'

However, it turns out that 'not dangerous' sensors also pose serious concerns to user privacy. For example, we can use the accelerometer or ambient light sensor of a cellphone to extract the user's screen lock pin~\cite{aviv2012practicality, spreitzer2018systematic}. The interactions between sensors pose a threat to our existing security models. it is difficult to anticipate the potential side channel attacks that existing sensors pose, let alone predict how additional sensors may create novel threat vectors.

Permissions have already become too complex to easily communicate to users what data is gathered and the potential consequences of its use or misuse.
