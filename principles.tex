Mixed reality technology has the potential to transform the way we interact with each other and the world around us. The best way to level out asymmetries of knowledge and power is to not allow them to form in the first place. This paper is an initial exploration into the challenges we face while we try to define what an ethical immersive future looks like. Based on the case studies presented here, I propose the following principles of building ethical software in mixed reality:

\begin{enumerate}
	\item Ask permission, not forgiveness
	\item Minimize tracking and fingerprinting via biometrics
	\item Empower individuals to define how they are perceived virtually
	\item Prioritize mechanisms for reporting harassment and blocking perpetrators
	\item Identify ways to incentivize the principle of least privilege
	\item Consider privacy a first-class requirement
	\item Be both transparent and accountable
\end{enumerate}

\subsection{Ask permission not forgiveness.} 
The current consent paradigm on the web and apps is based on notice and choice mechanisms, like privacy policies. However, users ignore these, because they are neither comprehensible nor useful~\cite{schaub}. MR has two particular challenges: firstly, it is difficult to convey the various privacy risks associated with different input sensors (let alone analyze the potential risks of interactions between sensors); secondly, the sheer number of input sensors makes notice fatigue a very real concern.

To combat permission fatigue, solutions like privacy assistants~\cite{liu2016follow} should be investigated. A key challenge in MR is how to effectively inform users of the privacy risks and the extent of information that can be leaked by MR data. However, some of this responsibility must lie with developers and implementers. For example, perhaps we should simply not expose certain data, like raw gaze data, to the web.

\subsection{Minimize tracking and fingerprinting via biometrics.}
While extensive and ubiquitous tracking and fingerprinting abound on both the web and apps today, immersive technologies present an enhanced concern---the ability to track users via biometrically derived data. MR devices use inputs such as gaze and gait for functionality, by necessity revealing these intrinsic user properties. Apple, for example, allows users to reset their advertising identifier, an ephemeral device identifier.\footnote{\url{https://support.apple.com/HT205223}} Unlike the advertising identifier, biometric data like gaze and gait are essentially set---we do not have conscious control over these powerful nonverbal identification means.	

This is not to say that non-biometric tracking is more acceptable than biometric-based trackers, simply that there is no `reset' button on a biometric-based tracker. Users can not consciously change the way their eyes or body move consistently (and in fact, such an effort would likely offer fingerprinting opportunities).

\subsection{Empower individuals to define how they are perceived virtually}
Social virtual spaces have emerged, including Altspace,\footnote{altvr.com} Hubs by Mozilla,\footnote{hubs.mozilla.com} RecRoom,\footnote{recroom.com} and Facebook Horizon,\footnote{\url{https://www.oculus.com/blog/introducing-facebook-horizon-a-new-social-vr-world-coming-to-oculus-quest-and-the-rift-platform-in-2020/}}
 prompting questions of how we represent individuals. How we choose to present ourselves in a social VR environment is a \emph{performative act of identity} and associates individuals with a particular selfhood, demographic, connections and identifications~\cite{cover2012performing}. It is therefore important that we empower users to define how they are perceived, particularly when using photorealistic or humanoid avatars.

As with any approach, there are drawbacks. Implicit biases follow individuals in the virtual realm~\cite{maloney}. Additionally, minority groups may want to carve out spaces for themselves and may require additional authentication methods.


\subsection{Prioritize mechanisms for reporting harassment and blocking perpetrators}
With a new communications medium comes new opportunities for harassment and abuse. Platforms need to prioritize mechanisms for protecting individuals, whether this be an easily-discoverable personal bubble to prevent unwanted virtual touching or muting and blocking tools. Different platforms will need to identify the mechanisms that are most suitable for their use cases. 

For example, in Hubs by Mozilla, if a user has the URL of a room, they are able to rejoin a room even if they were asked to leave or kicked out. In order to permanently ban a user, a new URL would need to be generated and distributed to the other users. Alternatively, Hubs has introduced Discord integration which allows them to tie individual's to an identity and require authentication for rooms. In this case, removing an individual from a linked Discord channel removes their ability to enter a Discord-authenticated Hubs room.\footnote{\url{https://blog.mozvr.com/creating-privacy-centric-virtual-spaces/}}

\subsection{Principle of least privilege.}
When possible, developers should default to the least privilege (the smallest amount of information requested and used) needed for functionality, progressively enhancing as needed. As noted above, there are no technical means to prevent misuse of input data like gaze while also enabling legitimate uses, so it is incumbent on developers and creators to make meaningful and respectful decisions.

\subsection{Consider privacy a first-class requirement}
It can be easy to defer privacy for more lucrative features, particularly when the data required to create MR experiences is inherently a privacy threat. Since this data is required to be processed,  developers and implementors need to make responsible privacy decisions. Examples of this principle in action include data minimization, storage limitation, and confidentiality. Another example would be to adapt the Fair Information Practice Principles (FIPPs) for MR~\cite{fipps}.

\subsection{Be transparent and accountable.}
While privacy policies ostensibly exist to inform users of data practices and use, they are primarily a tool for legal compliance, not transparency with users. Schaub et. al. describe the following hurdles to effective privacy notices: notice complexity, lack of choices, notice fatigue, and decoupled notices~\cite{schaub}. Instead of only providing privacy notices that are intended for regulatory compliance, we need notices that are intended to provide transparency to users in a useful and understandable way. Accountability pertains to what happens after consent is granted. For example, all active or granted permissions should be easy to inspect and easy to change.
